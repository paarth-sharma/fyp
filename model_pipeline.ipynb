{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Setup and imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PaarthSharma\\.vscode\\fyp\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "import wikipedia\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()  # Loads variables from .env into the environment\n",
    "\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "GOOGLE_API_KEY = os.getenv(\"API_KEY\", \"\")\n",
    "GOOGLE_CSE_ID = os.getenv(\"SEARCH_ENGINE_ID\", \"\")\n",
    "\n",
    "# For demonstration only; adapt as needed\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. External context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_wikipedia_context(query, max_chars=1000, timeout=5):\n",
    "    \"\"\"\n",
    "    Simple Wikipedia snippet fetch with python-wikipedia library.\n",
    "    'query' can be a title or search term.\n",
    "    'max_chars' is how many characters to return from the summary.\n",
    "    'timeout' is a naive approach for demonstration.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        wikipedia.set_lang(\"en\")\n",
    "        wikipedia.set_rate_limiting(True)\n",
    "        # We'll do a simple approach, ignoring advanced concurrency/timeouts\n",
    "        page_titles = wikipedia.search(query, results=1)\n",
    "        if not page_titles:\n",
    "            return \"\"\n",
    "        page_title = page_titles[0]\n",
    "        summary = wikipedia.summary(page_title, sentences=2)\n",
    "        return summary[:max_chars]\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Wikipedia fetch error: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def fetch_google_context(query, api_key=None, cse_id=None, max_chars=1000, timeout=5):\n",
    "    \"\"\"\n",
    "    Demonstration of using a Google Custom Search Engine (CSE).\n",
    "    'api_key' and 'cse_id' come from .env => (API_KEY, SEARCH_ENGINE_ID).\n",
    "    Returns top snippet or empty string if no results found.\n",
    "    \"\"\"\n",
    "    if not api_key or not cse_id:\n",
    "        return \"\"\n",
    "\n",
    "    base_url = \"https://www.googleapis.com/customsearch/v1\"\n",
    "    params = {\n",
    "        \"key\": api_key,\n",
    "        \"cx\": cse_id,\n",
    "        \"q\": query\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        r = requests.get(base_url, params=params, timeout=timeout)\n",
    "        if r.status_code == 200:\n",
    "            data = r.json()\n",
    "            items = data.get(\"items\", [])\n",
    "            if not items:\n",
    "                return \"\"\n",
    "            snippet = items[0].get(\"snippet\", \"\")\n",
    "            return snippet[:max_chars]\n",
    "        else:\n",
    "            print(f\"[WARN] Google search error: status={r.status_code}\")\n",
    "            return \"\"\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Google search failed: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def gather_external_context_from_title(json_file, api_key=None, cse_id=None):\n",
    "    \"\"\"\n",
    "    1. Load the JSON data, extract a potential title (first sentence).\n",
    "    2. Use fetch_wikipedia_context or fetch_google_context to get external info.\n",
    "    3. Return combined text snippet.\n",
    "    \"\"\"\n",
    "    with open(json_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Heuristic: let's look at the json and find the doc title to be the 4th sentence\n",
    "    if data[\"original_sentences\"]:\n",
    "        potential_title = data[\"original_sentences\"][3]\n",
    "    else:\n",
    "        potential_title = \"Untitled Document\"\n",
    "\n",
    "    # Wikipedia\n",
    "    wiki_context = fetch_wikipedia_context(potential_title, max_chars=1000)\n",
    "\n",
    "    # Google\n",
    "    google_context = fetch_google_context(potential_title, api_key=api_key, cse_id=cse_id, max_chars=1000)\n",
    "\n",
    "    combined_context = wiki_context + \"\\n\\n\" + google_context\n",
    "    return combined_context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_training_data(json_file):\n",
    "    \"\"\"\n",
    "    Return lists of original sentences (train) and censored sentences (test).\n",
    "    \"\"\"\n",
    "    with open(json_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    original_sents = data[\"original_sentences\"]\n",
    "    censored_sents = data[\"censored_sentences\"]\n",
    "    return original_sents, censored_sents\n",
    "\n",
    "def create_masked_texts(original_sents):\n",
    "    \"\"\"\n",
    "    We create masked versions of the sentences for a naive MLM approach.\n",
    "    Example: any word that starts uppercase we replace with [MASK].\n",
    "    Also store ground truths so we know what we replaced.\n",
    "    \"\"\"\n",
    "    masked_texts = []\n",
    "    ground_truths = []\n",
    "\n",
    "    for sent in original_sents:\n",
    "        words = sent.split()\n",
    "        new_words = []\n",
    "        truth_words = []\n",
    "        for w in words:\n",
    "            # Simple heuristic: if w starts uppercase & length>3 => mask\n",
    "            # You can do more advanced checks or use spaCy to detect named entities\n",
    "            if w[0].isupper() and len(w) > 3:\n",
    "                new_words.append(\"[MASK]\")\n",
    "                truth_words.append(w)\n",
    "            else:\n",
    "                new_words.append(w)\n",
    "                truth_words.append(None)\n",
    "        masked_sent = \" \".join(new_words)\n",
    "        masked_texts.append(masked_sent)\n",
    "        ground_truths.append(truth_words)\n",
    "\n",
    "    return masked_texts, ground_truths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. BERT model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "\n",
    "def prepare_bert_model():\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\")\n",
    "    model.to(DEVICE)\n",
    "    return tokenizer, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Fine tuning the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def fine_tune_bert_maskedLM(tokenizer, model, masked_texts, epochs=1, batch_size=4):\n",
    "    \"\"\"\n",
    "    Example training loop for masked language modeling.\n",
    "    This is a minimal approach, for demonstration only.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        batch_start = 0\n",
    "        while batch_start < len(masked_texts):\n",
    "            batch_end = batch_start + batch_size\n",
    "            batch_sents = masked_texts[batch_start:batch_end]\n",
    "\n",
    "            inputs = tokenizer(batch_sents, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "            input_ids = inputs[\"input_ids\"].to(DEVICE)\n",
    "            attention_mask = inputs[\"attention_mask\"].to(DEVICE)\n",
    "\n",
    "            # BERTForMaskedLM expects labels=input_ids for teacher-forcing\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=input_ids)\n",
    "            loss = outputs.loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            print(f\"Epoch {epoch} | Batch {batch_start} Loss={loss.item():.4f}\")\n",
    "            batch_start += batch_size\n",
    "\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Inference and top-K predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_predictions_for_masked_sentence(tokenizer, model, masked_sentence, top_k=5):\n",
    "    model.eval()\n",
    "    inputs = tokenizer(masked_sentence, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"].to(DEVICE)\n",
    "    attention_mask = inputs[\"attention_mask\"].to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    \n",
    "    logits = outputs.logits  # [batch_size, seq_len, vocab_size]\n",
    "    mask_token_index = (input_ids == tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]\n",
    "\n",
    "    predictions = {}\n",
    "    for idx in mask_token_index:\n",
    "        idx = idx.item()\n",
    "        logits_for_mask = logits[0, idx]\n",
    "        probs = F.softmax(logits_for_mask, dim=0)\n",
    "        top_probs, top_ids = probs.topk(top_k)\n",
    "\n",
    "        predicted_tokens = [tokenizer.convert_ids_to_tokens(int(i)) for i in top_ids]\n",
    "        predicted_scores = [float(tp) for tp in top_probs]\n",
    "\n",
    "        predictions[idx] = list(zip(predicted_tokens, predicted_scores))\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Putting everything together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] Wikipedia fetch error: Page id \"structured equation modeling\" does not match any pages. Try another id!\n",
      "[WARN] Google search error: status=403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Batch 0 Loss=9.8682\n",
      "Epoch 0 | Batch 2 Loss=5.9569\n",
      "Epoch 0 | Batch 4 Loss=6.5115\n",
      "Epoch 0 | Batch 6 Loss=6.5036\n",
      "Epoch 0 | Batch 8 Loss=8.4266\n",
      "Epoch 0 | Batch 10 Loss=7.5029\n",
      "Epoch 0 | Batch 12 Loss=6.4291\n",
      "Epoch 0 | Batch 14 Loss=3.2505\n",
      "Epoch 0 | Batch 16 Loss=4.9005\n",
      "Epoch 0 | Batch 18 Loss=7.0371\n",
      "Epoch 0 | Batch 20 Loss=8.4356\n",
      "Epoch 0 | Batch 22 Loss=5.3258\n",
      "Epoch 0 | Batch 24 Loss=4.0817\n",
      "Epoch 0 | Batch 26 Loss=3.6666\n",
      "Epoch 0 | Batch 28 Loss=2.6724\n",
      "Epoch 0 | Batch 30 Loss=3.9941\n",
      "Epoch 0 | Batch 32 Loss=7.8576\n",
      "Epoch 0 | Batch 34 Loss=7.1682\n",
      "Epoch 0 | Batch 36 Loss=6.0278\n",
      "Epoch 0 | Batch 38 Loss=3.9803\n",
      "Epoch 0 | Batch 40 Loss=5.5448\n",
      "Epoch 0 | Batch 42 Loss=5.5054\n",
      "Epoch 0 | Batch 44 Loss=4.0677\n",
      "Epoch 0 | Batch 46 Loss=3.8805\n",
      "Epoch 0 | Batch 48 Loss=3.4331\n",
      "Epoch 0 | Batch 50 Loss=4.9956\n",
      "Epoch 0 | Batch 52 Loss=4.5139\n",
      "Epoch 0 | Batch 54 Loss=6.8744\n",
      "Epoch 0 | Batch 56 Loss=8.7369\n",
      "Epoch 0 | Batch 58 Loss=5.0363\n",
      "Epoch 0 | Batch 60 Loss=4.6140\n",
      "Epoch 0 | Batch 62 Loss=5.5696\n",
      "Epoch 0 | Batch 64 Loss=6.3290\n",
      "Epoch 0 | Batch 66 Loss=6.9598\n",
      "Epoch 0 | Batch 68 Loss=3.6791\n",
      "Epoch 0 | Batch 70 Loss=4.2575\n",
      "Epoch 0 | Batch 72 Loss=4.5599\n",
      "Epoch 0 | Batch 74 Loss=6.0186\n",
      "Epoch 0 | Batch 76 Loss=5.4191\n",
      "Epoch 0 | Batch 78 Loss=3.2521\n",
      "Epoch 0 | Batch 80 Loss=5.3873\n",
      "Epoch 0 | Batch 82 Loss=5.2869\n",
      "Epoch 0 | Batch 84 Loss=5.3171\n",
      "Epoch 0 | Batch 86 Loss=4.5265\n",
      "Epoch 0 | Batch 88 Loss=5.8851\n",
      "Epoch 0 | Batch 90 Loss=4.1038\n",
      "Epoch 0 | Batch 92 Loss=4.2463\n",
      "Epoch 0 | Batch 94 Loss=4.1446\n",
      "Epoch 0 | Batch 96 Loss=4.9711\n",
      "Epoch 0 | Batch 98 Loss=6.0347\n",
      "Epoch 0 | Batch 100 Loss=2.2816\n",
      "Epoch 0 | Batch 102 Loss=3.9481\n",
      "Epoch 0 | Batch 104 Loss=3.4508\n",
      "Epoch 0 | Batch 106 Loss=5.8359\n",
      "Epoch 0 | Batch 108 Loss=2.4265\n",
      "Epoch 0 | Batch 110 Loss=3.8420\n",
      "Epoch 0 | Batch 112 Loss=5.1625\n",
      "Epoch 0 | Batch 114 Loss=2.5634\n",
      "Epoch 0 | Batch 116 Loss=4.2261\n",
      "Epoch 0 | Batch 118 Loss=5.2221\n",
      "Epoch 0 | Batch 120 Loss=4.8135\n",
      "Epoch 0 | Batch 122 Loss=1.8029\n",
      "Epoch 0 | Batch 124 Loss=3.2711\n",
      "Epoch 0 | Batch 126 Loss=4.3067\n",
      "Epoch 0 | Batch 128 Loss=4.6568\n",
      "Epoch 0 | Batch 130 Loss=1.8511\n",
      "Epoch 0 | Batch 132 Loss=3.6368\n",
      "Epoch 0 | Batch 134 Loss=4.6098\n",
      "Epoch 0 | Batch 136 Loss=4.7271\n",
      "Epoch 0 | Batch 138 Loss=2.0331\n",
      "Epoch 0 | Batch 140 Loss=1.9286\n",
      "Epoch 0 | Batch 142 Loss=2.6032\n",
      "Epoch 0 | Batch 144 Loss=4.1574\n",
      "Epoch 0 | Batch 146 Loss=3.6790\n",
      "Epoch 0 | Batch 148 Loss=2.8660\n",
      "Epoch 0 | Batch 150 Loss=3.1472\n",
      "Epoch 0 | Batch 152 Loss=3.6732\n",
      "Epoch 0 | Batch 154 Loss=2.3506\n",
      "Epoch 0 | Batch 156 Loss=3.5025\n",
      "Epoch 0 | Batch 158 Loss=2.5909\n",
      "Epoch 0 | Batch 160 Loss=2.8658\n",
      "Epoch 0 | Batch 162 Loss=2.2422\n",
      "Epoch 0 | Batch 164 Loss=2.2434\n",
      "Epoch 0 | Batch 166 Loss=2.9725\n",
      "Epoch 0 | Batch 168 Loss=2.9697\n",
      "Epoch 0 | Batch 170 Loss=1.8449\n",
      "Epoch 0 | Batch 172 Loss=3.2747\n",
      "Epoch 0 | Batch 174 Loss=1.6753\n",
      "Epoch 0 | Batch 176 Loss=2.5831\n",
      "Epoch 0 | Batch 178 Loss=1.0853\n",
      "Epoch 0 | Batch 180 Loss=2.6302\n",
      "Epoch 0 | Batch 182 Loss=2.0287\n",
      "Epoch 0 | Batch 184 Loss=2.4863\n",
      "Epoch 0 | Batch 186 Loss=3.4037\n",
      "Epoch 0 | Batch 188 Loss=2.5612\n",
      "Epoch 0 | Batch 190 Loss=1.9144\n",
      "Epoch 0 | Batch 192 Loss=3.0580\n",
      "Epoch 0 | Batch 194 Loss=2.0451\n",
      "Epoch 0 | Batch 196 Loss=1.2158\n",
      "Epoch 0 | Batch 198 Loss=2.9576\n",
      "Epoch 0 | Batch 200 Loss=1.7007\n",
      "Epoch 0 | Batch 202 Loss=1.8782\n",
      "Epoch 0 | Batch 204 Loss=2.4529\n",
      "Epoch 0 | Batch 206 Loss=2.5406\n",
      "Epoch 0 | Batch 208 Loss=1.6191\n",
      "Epoch 0 | Batch 210 Loss=2.8124\n",
      "Epoch 0 | Batch 212 Loss=2.0846\n",
      "Epoch 0 | Batch 214 Loss=1.4211\n",
      "Epoch 0 | Batch 216 Loss=2.5650\n",
      "Epoch 0 | Batch 218 Loss=2.1779\n",
      "Epoch 0 | Batch 220 Loss=1.9212\n",
      "Epoch 0 | Batch 222 Loss=2.5479\n",
      "Epoch 0 | Batch 224 Loss=1.9333\n",
      "Epoch 0 | Batch 226 Loss=1.7308\n",
      "Epoch 0 | Batch 228 Loss=1.4547\n",
      "Epoch 0 | Batch 230 Loss=1.4790\n",
      "Epoch 0 | Batch 232 Loss=1.5480\n",
      "Epoch 0 | Batch 234 Loss=1.1757\n",
      "Epoch 0 | Batch 236 Loss=1.0447\n",
      "Epoch 0 | Batch 238 Loss=1.3850\n",
      "Epoch 0 | Batch 240 Loss=1.9610\n",
      "Epoch 0 | Batch 242 Loss=0.5552\n",
      "Epoch 0 | Batch 244 Loss=1.5823\n",
      "\n",
      "=== TOP PREDICTIONS FOR TEST MASK ===\n",
      "{4: [('[PAD]', 0.007729722186923027), ('humans', 0.007536450866609812), ('colonists', 0.007168133743107319), ('vampires', 0.006523325107991695), ('combatants', 0.005701154470443726)], 7: [('[PAD]', 0.0090608736500144), ('colonists', 0.006924247834831476), ('guerrillas', 0.0062456922605633736), ('allies', 0.00620289659127593), ('taliban', 0.004959071520715952)]}\n",
      "\n",
      "Censored Sentence 0: i\n",
      "\n",
      "APPROVED FOR RELEASE - CIA INFO DATE.\n",
      "Mask Predictions: {}\n",
      "\n",
      "Censored Sentence 1: 15-Oct-2012\n",
      "\n",
      "13 October 1973\n",
      "Arab-Israeli Hostilities:.\n",
      "Mask Predictions: {}\n",
      "\n",
      "Censored Sentence 2: Two Scenarios\n",
      "\n",
      "The Arab-Israeli war is approaching a decisive turning\n",
      "point.\n",
      "Mask Predictions: {}\n"
     ]
    }
   ],
   "source": [
    "def main(json_file=\"./data/processed/document_1_processed.json\"):\n",
    "    # Step A: gather external context\n",
    "    external_context = gather_external_context_from_title(\n",
    "        json_file,\n",
    "        api_key=GOOGLE_API_KEY,\n",
    "        cse_id=GOOGLE_CSE_ID\n",
    "    )\n",
    "\n",
    "    # Step B: load data\n",
    "    original_sents, censored_sents = load_training_data(json_file)\n",
    "\n",
    "    # Merge external context\n",
    "    # We treat the external context as an additional \"sentence\" for training\n",
    "    original_plus_context = original_sents + [external_context]\n",
    "\n",
    "    # Step C: create masked data\n",
    "    masked_texts, ground_truths = create_masked_texts(original_plus_context)\n",
    "\n",
    "    # Step D: prepare & fine-tune BERT\n",
    "    tokenizer, model = prepare_bert_model()\n",
    "    model = fine_tune_bert_maskedLM(tokenizer, model, masked_texts, epochs=1, batch_size=2)\n",
    "\n",
    "    # Step E: test with a random masked sentence\n",
    "    test_masked = \"areas where the [MASK] and the [MASK] attacked.\"\n",
    "    top_preds = get_top_predictions_for_masked_sentence(tokenizer, model, test_masked, top_k=5)\n",
    "    print(\"\\n=== TOP PREDICTIONS FOR TEST MASK ===\")\n",
    "    print(top_preds)\n",
    "\n",
    "    # Also see how it tries to reconstruct a censored sentence\n",
    "    # We do a quick hack: replace [REDACTED] with [MASK]\n",
    "    for i, cens in enumerate(censored_sents[:3]):\n",
    "        test_sent_masked = cens.replace(\"[REDACTED]\", tokenizer.mask_token)\n",
    "        results = get_top_predictions_for_masked_sentence(tokenizer, model, test_sent_masked, top_k=5)\n",
    "        print(f\"\\nCensored Sentence {i}: {cens}\")\n",
    "        print(f\"Mask Predictions: {results}\")\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
