{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Preprocessing and Redaction Detection\n",
    "\n",
    "Extract Text: Use PyPDF2 or pdfplumber to extract text from the PDF.\n",
    "\n",
    "Identify Redactions: Detect redacted sections (e.g., lines with \"SECRET\" or declassification notes) using regex patterns like r\"SECRET\\s+.*\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 215 sentences from JSON.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Read the entire JSON dataset\n",
    "with open(\"./data/processed/document_1_processed.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "sentences = []\n",
    "\n",
    "for item in data:\n",
    "    # Pull the already-processed text from the JSON\n",
    "    text = item[\"raw_text\"]\n",
    "\n",
    "    # Minor cleanup of stray characters\n",
    "    text = text.replace(\"\\n\", \" \").replace(\"\\x0c\", \" \").strip()\n",
    "    # Optional custom cleanup, if you have a function:\n",
    "    # text = remove_weird_characters(text)\n",
    "\n",
    "    # Split into sentences however you did before:\n",
    "    # Replace this with your own sentence-splitting method if needed\n",
    "    these_sentences = text.split(\". \")  # Example only\n",
    "\n",
    "    # Accumulate all sentences\n",
    "    sentences.extend(these_sentences)\n",
    "\n",
    "print(f\"Loaded {len(sentences)} sentences from JSON.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import spacy\n",
    "import requests\n",
    "import wikipedia\n",
    "from transformers import BertForMaskedLM, BertTokenizer\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "class HybridReconstructor:\n",
    "    def __init__(self):\n",
    "        # Initialize BERT\n",
    "        self.bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "        self.bert_model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\")\n",
    "        \n",
    "        # Initialize spaCy NER\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "        \n",
    "        # Google API credentials\n",
    "        self.API_KEY = os.getenv(\"API_KEY\")\n",
    "        self.SEARCH_ENGINE_ID = os.getenv(\"SEARCH_ENGINE_ID\")\n",
    "\n",
    "    def predict_masked_tokens(self, text: str, top_k: int = 5) -> List[str]:\n",
    "        \"\"\"Get top-k predictions for masked tokens using BERT\"\"\"\n",
    "        inputs = self.bert_tokenizer(text, return_tensors=\"pt\")\n",
    "        outputs = self.bert_model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        masked_indices = (inputs.input_ids == self.bert_tokenizer.mask_token_id).nonzero(as_tuple=True)[1]\n",
    "        if not masked_indices.numel():  # No [MASK] tokens found\n",
    "            return [\"[MISSING MASK TOKEN]\"]\n",
    "        \n",
    "        predictions = []\n",
    "        \n",
    "        for idx in masked_indices:\n",
    "            probs = torch.softmax(logits[0, idx], dim=0)\n",
    "            top_tokens = torch.topk(probs, top_k).indices.tolist()\n",
    "            predictions.extend([\n",
    "                self.bert_tokenizer.decode([token]) for token in top_tokens\n",
    "            ])\n",
    "        return predictions\n",
    "\n",
    "    def get_ner_constraints(self, text: str) -> List[str]:\n",
    "        \"\"\"Extract entity types from context\"\"\"\n",
    "        doc = self.nlp(text)\n",
    "        return [ent.label_ for ent in doc.ents]\n",
    "\n",
    "    def get_external_context(self, query: str, num_results: int = 3) -> str:\n",
    "        \"\"\"Retrieve external context with validation\"\"\"\n",
    "        context = []\n",
    "        \n",
    "        # Wikipedia\n",
    "        try:\n",
    "            wiki_summary = wikipedia.summary(query, sentences=2)\n",
    "            if len(wiki_summary.split()) > 3:  # Ensure meaningful content\n",
    "                context.append(wiki_summary)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Google Search\n",
    "        try:\n",
    "            url = f\"https://www.googleapis.com/customsearch/v1?key={self.API_KEY}&cx={self.SEARCH_ENGINE_ID}&q={query}\"\n",
    "            response = requests.get(url).json()\n",
    "            for item in response.get(\"items\", [])[:num_results]:\n",
    "                snippet = item.get(\"snippet\", \"\")\n",
    "                if len(snippet.split()) > 3:\n",
    "                    context.append(snippet)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "        return \" \".join(context) if context else \"oil embargo arab production cut\"  # Fallback\n",
    "\n",
    "    def rank_predictions(self, predictions: List[str], context: str, entity_types: List[str]) -> List[Tuple[str, float]]:\n",
    "        \"\"\"Rank predictions with robust error handling\"\"\"\n",
    "        from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "        from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "        # 1. Filter predictions with NER constraints\n",
    "        valid_preds = []\n",
    "        for pred in predictions:\n",
    "            doc = self.nlp(pred)\n",
    "            if any(ent.label_ in entity_types for ent in doc.ents) or not entity_types:\n",
    "                valid_preds.append(pred)\n",
    "\n",
    "        # 2. Exit early if no valid predictions\n",
    "        if not valid_preds:\n",
    "            return [(pred, 0.0) for pred in predictions]  # Return original predictions\n",
    "\n",
    "        # 3. Ensure non-empty context\n",
    "        context = context.strip() or \"oil embargo arab production cut\"\n",
    "\n",
    "        try:\n",
    "            # 4. Vectorize with TF-IDF\n",
    "            vectorizer = TfidfVectorizer(min_df=1)  # Allow single-word docs\n",
    "            context_vec = vectorizer.fit_transform([context])\n",
    "            pred_vecs = vectorizer.transform(valid_preds)\n",
    "            \n",
    "            # 5. Calculate similarity scores\n",
    "            similarities = cosine_similarity(context_vec, pred_vecs).flatten()\n",
    "            ranked = sorted(zip(valid_preds, similarities), key=lambda x: x[1], reverse=True)\n",
    "        except ValueError:\n",
    "            ranked = [(pred, 0.0) for pred in valid_preds]\n",
    "\n",
    "        return ranked\n",
    "\n",
    "    def reconstruct(self, text: str) -> str:\n",
    "        \"\"\"Final reconstruction with comprehensive error handling\"\"\"\n",
    "        try:\n",
    "            # Step 1: Get NER constraints\n",
    "            entity_types = self.get_ner_constraints(text)\n",
    "            \n",
    "            # Step 2: Get external context\n",
    "            context = self.get_external_context(text)\n",
    "            \n",
    "            # Step 3: Get BERT predictions\n",
    "            predictions = self.predict_masked_tokens(text)\n",
    "            if not predictions:  # Handle empty predictions\n",
    "                return \"[NO BERT PREDICTIONS]\"\n",
    "            \n",
    "            # Step 4: Rank predictions\n",
    "            ranked = self.rank_predictions(predictions, context, entity_types)\n",
    "            \n",
    "            # Step 5: Return best available prediction\n",
    "            if ranked:\n",
    "                return ranked[0][0]\n",
    "            return predictions[0]  # Fallback to first BERT prediction\n",
    "        except Exception as e:\n",
    "            return f\"[RECONSTRUCTION ERROR: {str(e)}]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_redacted_text(pdf_path):\n",
    "    # Step 1: Extract redacted sections\n",
    "    redacted = extract_redacted_sections(pdf_path)\n",
    "    \n",
    "    # Step 2: Initialize reconstructor\n",
    "    reconstructor = HybridReconstructor()\n",
    "    \n",
    "    # Step 3: Reconstruct each section\n",
    "    reconstructed = []\n",
    "    for section in redacted:\n",
    "        result = reconstructor.reconstruct(section)\n",
    "        reconstructed.append({\n",
    "            \"original\": section,\n",
    "            \"prediction\": result,\n",
    "            \"entities\": reconstructor.get_ner_constraints(section),\n",
    "            \"external_context\": reconstructor.get_external_context(section)\n",
    "        })\n",
    "    return reconstructed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MISSING MASK TOKEN]\n"
     ]
    }
   ],
   "source": [
    "# Example Usage\n",
    "redacted_text = \"On 17 October 1973, ****** decided to cut oil production by 5% monthly.\"\n",
    "results = reconstruct_redacted_text(\"./data/input/document_1.pdf\")\n",
    "print(results[0][\"prediction\"])  # Should show context-aware reconstruction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
