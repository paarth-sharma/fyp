{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Setup and imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "import wikipedia\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()  # Loads variables from .env into the environment\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from transformers import BertTokenizer, BertForMaskedLM, pipeline\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "GOOGLE_API_KEY = os.getenv(\"API_KEY\", \"\")\n",
    "GOOGLE_CSE_ID = os.getenv(\"SEARCH_ENGINE_ID\", \"\")\n",
    "\n",
    "# For demonstration only; adapt as needed\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. External context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a global BERT tokenizer\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def tokenize_redacted_context(redacted_context_list):\n",
    "    \"\"\"\n",
    "    Takes a list of redacted sentences (strings).\n",
    "    Returns a list of dicts with:\n",
    "      - 'original_text'\n",
    "      - 'simple_tokens'  (for optional debugging)\n",
    "      - 'input_ids'      (BERT-compatible IDs)\n",
    "      - 'attention_mask' (BERT mask)\n",
    "    \"\"\"\n",
    "    tokenized_result = []\n",
    "    for sentence in redacted_context_list:\n",
    "        # Simple NLTK-based tokenization for debugging/log\n",
    "        simple_tokens = word_tokenize(sentence)\n",
    "        \n",
    "        # BERT-based tokenization\n",
    "        bert_encoded = bert_tokenizer.encode_plus(\n",
    "            sentence,\n",
    "            add_special_tokens=True,\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        tokenized_result.append({\n",
    "            \"original_text\": sentence,\n",
    "            \"simple_tokens\": simple_tokens,\n",
    "            \"input_ids\": bert_encoded[\"input_ids\"],\n",
    "            \"attention_mask\": bert_encoded[\"attention_mask\"]\n",
    "        })\n",
    "    return tokenized_result\n",
    "\n",
    "# def fetch_wikipedia_context(query, max_chars=1000, timeout=5):\n",
    "#     \"\"\"\n",
    "#     Simple Wikipedia snippet fetch with python-wikipedia library.\n",
    "#     'query' can be a title or search term.\n",
    "#     'max_chars' is how many characters to return from the summary.\n",
    "#     'timeout' is a naive approach for demonstration.\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         wikipedia.set_lang(\"en\")\n",
    "#         wikipedia.set_rate_limiting(True)\n",
    "#         # We'll do a simple approach, ignoring advanced concurrency/timeouts\n",
    "#         page_titles = wikipedia.search(query, results=1)\n",
    "#         if not page_titles:\n",
    "#             return \"\"\n",
    "#         page_title = page_titles[0]\n",
    "#         summary = wikipedia.summary(page_title, sentences=2)\n",
    "#         return summary[:max_chars]\n",
    "#     except Exception as e:\n",
    "#         print(f\"[WARN] Wikipedia fetch error: {e}\")\n",
    "#         return \"\"\n",
    "\n",
    "# def fetch_google_context(query, api_key=None, cse_id=None, max_chars=1000, timeout=5):\n",
    "#     \"\"\"\n",
    "#     Demonstration of using a Google Custom Search Engine (CSE).\n",
    "#     'api_key' and 'cse_id' come from .env => (API_KEY, SEARCH_ENGINE_ID).\n",
    "#     Returns top snippet or empty string if no results found.\n",
    "#     \"\"\"\n",
    "#     if not api_key or not cse_id:\n",
    "#         return \"\"\n",
    "\n",
    "#     base_url = \"https://www.googleapis.com/customsearch/v1\"\n",
    "#     params = {\n",
    "#         \"key\": api_key,\n",
    "#         \"cx\": cse_id,\n",
    "#         \"q\": query\n",
    "#     }\n",
    "\n",
    "#     try:\n",
    "#         r = requests.get(base_url, params=params, timeout=timeout)\n",
    "#         if r.status_code == 200:\n",
    "#             data = r.json()\n",
    "#             items = data.get(\"items\", [])\n",
    "#             if not items:\n",
    "#                 return \"\"\n",
    "#             snippet = items[0].get(\"snippet\", \"\")\n",
    "#             return snippet[:max_chars]\n",
    "#         else:\n",
    "#             print(f\"[WARN] Google search error: status={r.status_code}\")\n",
    "#             return \"\"\n",
    "#     except Exception as e:\n",
    "#         print(f\"[WARN] Google search failed: {e}\")\n",
    "#         return \"\"\n",
    "\n",
    "# def gather_external_context_from_title(json_file, api_key=None, cse_id=None):\n",
    "#     \"\"\"\n",
    "#     1. Load the JSON data, extract a potential title (first sentence).\n",
    "#     2. Use fetch_wikipedia_context or fetch_google_context to get external info.\n",
    "#     3. Return combined text snippet.\n",
    "#     \"\"\"\n",
    "#     with open(json_file, \"r\", encoding=\"utf-8\") as f:\n",
    "#         data = json.load(f)\n",
    "\n",
    "#     # Heuristic: let's look at the json and find the doc title to be the 4th sentence\n",
    "#     if data[\"original_sentences\"]:\n",
    "#         potential_title = data[\"original_sentences\"][3]\n",
    "#     else:\n",
    "#         potential_title = \"Untitled Document\"\n",
    "\n",
    "#     # Wikipedia\n",
    "#     wiki_context = fetch_wikipedia_context(potential_title, max_chars=1000)\n",
    "\n",
    "#     # Google\n",
    "#     google_context = fetch_google_context(potential_title, api_key=api_key, cse_id=cse_id, max_chars=1000)\n",
    "\n",
    "#     combined_context = wiki_context + \"\\n\\n\" + google_context\n",
    "#     return combined_context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "# def load_training_data(json_file):\n",
    "#     \"\"\"\n",
    "#     Return lists of original sentences (train) and censored sentences (test).\n",
    "#     \"\"\"\n",
    "#     with open(json_file, \"r\", encoding=\"utf-8\") as f:\n",
    "#         data = json.load(f)\n",
    "#     original_sents = data[\"original_sentences\"]\n",
    "#     censored_sents = data[\"censored_sentences\"]\n",
    "#     return original_sents, censored_sents\n",
    "\n",
    "# def create_masked_texts(original_sents):\n",
    "#     \"\"\"\n",
    "#     We create masked versions of the sentences for a naive MLM approach.\n",
    "#     Example: any word that starts uppercase we replace with [MASK].\n",
    "#     Also store ground truths so we know what we replaced.\n",
    "#     \"\"\"\n",
    "#     masked_texts = []\n",
    "#     ground_truths = []\n",
    "\n",
    "#     for sent in original_sents:\n",
    "#         words = sent.split()\n",
    "#         new_words = []\n",
    "#         truth_words = []\n",
    "#         for w in words:\n",
    "#             # Simple heuristic: if w starts uppercase & length>3 => mask\n",
    "#             # You can do more advanced checks or use spaCy to detect named entities\n",
    "#             if w[0].isupper() and len(w) > 3:\n",
    "#                 new_words.append(\"[MASK]\")\n",
    "#                 truth_words.append(w)\n",
    "#             else:\n",
    "#                 new_words.append(w)\n",
    "#                 truth_words.append(None)\n",
    "#         masked_sent = \" \".join(new_words)\n",
    "#         masked_texts.append(masked_sent)\n",
    "#         ground_truths.append(truth_words)\n",
    "\n",
    "#     return masked_texts, ground_truths\n",
    "\n",
    "# Summarization pipeline\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "def summarize_if_needed(text_in, max_bert_length=512):\n",
    "    \"\"\"\n",
    "    Summarizes text_in if it produces more than 'max_bert_length'\n",
    "    tokens for BERT. Otherwise returns it unchanged.\n",
    "    \"\"\"\n",
    "    tokens = bert_tokenizer.encode(text_in, add_special_tokens=False)\n",
    "    if len(tokens) <= max_bert_length:\n",
    "        return text_in  # No summarization required\n",
    "    \n",
    "    # Summarize\n",
    "    summary_out = summarizer(\n",
    "        text_in,\n",
    "        max_length=150,  # tune as needed\n",
    "        min_length=40,\n",
    "        do_sample=False\n",
    "    )\n",
    "    return summary_out[0][\"summary_text\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. BERT model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# from transformers import BertTokenizer, BertForMaskedLM\n",
    "\n",
    "# def prepare_bert_model():\n",
    "#     tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "#     model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\")\n",
    "#     model.to(DEVICE)\n",
    "#     return tokenizer, model\n",
    "\n",
    "# Load the BERT masked LM\n",
    "bert_masked_model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "##############################################\n",
    "# External context gathering: Wikipedia example\n",
    "##############################################\n",
    "# Make sure you install the 'wikipedia' package:\n",
    "#   pip install wikipedia\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Fine tuning the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import torch\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# def fine_tune_bert_maskedLM(tokenizer, model, masked_texts, epochs=1, batch_size=4):\n",
    "#     \"\"\"\n",
    "#     Example training loop for masked language modeling.\n",
    "#     This is a minimal approach, for demonstration only.\n",
    "#     \"\"\"\n",
    "#     model.train()\n",
    "#     optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "#     for epoch in range(epochs):\n",
    "#         batch_start = 0\n",
    "#         while batch_start < len(masked_texts):\n",
    "#             batch_end = batch_start + batch_size\n",
    "#             batch_sents = masked_texts[batch_start:batch_end]\n",
    "\n",
    "#             inputs = tokenizer(batch_sents, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "#             input_ids = inputs[\"input_ids\"].to(DEVICE)\n",
    "#             attention_mask = inputs[\"attention_mask\"].to(DEVICE)\n",
    "\n",
    "#             # BERTForMaskedLM expects labels=input_ids for teacher-forcing\n",
    "#             outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=input_ids)\n",
    "#             loss = outputs.loss\n",
    "\n",
    "#             optimizer.zero_grad()\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "#             print(f\"Epoch {epoch} | Batch {batch_start} Loss={loss.item():.4f}\")\n",
    "#             batch_start += batch_size\n",
    "\n",
    "#     model.eval()\n",
    "#     return model\n",
    "\n",
    "def gather_wikipedia_context(search_terms, max_chars=1000):\n",
    "    \"\"\"\n",
    "    Searches Wikipedia for each term in 'search_terms' (a list of strings).\n",
    "    Concatenates summaries up to 'max_chars' to avoid going overly long.\n",
    "    \"\"\"\n",
    "    combined = \"\"\n",
    "    for term in search_terms:\n",
    "        try:\n",
    "            # Get summary\n",
    "            summary_txt = wikipedia.summary(term, sentences=3)\n",
    "            # Accumulate\n",
    "            if len(combined) + len(summary_txt) <= max_chars:\n",
    "                combined += \" \" + summary_txt\n",
    "            else:\n",
    "                break\n",
    "        except Exception as e:\n",
    "            # If there's an error (page not found, disambiguation, etc.), skip\n",
    "            print(f\"[WARN] Could not retrieve Wikipedia for term '{term}' -> {e}\")\n",
    "            continue\n",
    "    return combined.strip()\n",
    "\n",
    "def iterative_decensoring(redacted_text, wikipedia_search_terms=None, epochs=3):\n",
    "    \"\"\"\n",
    "    Iteratively replace [REDACTED] placeholders by:\n",
    "      1) Optionally gather external context from Wikipedia (if search terms given).\n",
    "      2) Combine external context + redacted text -> single string\n",
    "      3) Summarize if needed\n",
    "      4) Replace 1st occurrence of [REDACTED] with [MASK]\n",
    "      5) BERT Masked LM to predict the masked token\n",
    "      6) Insert predicted token\n",
    "      7) Repeat for 'epochs' or until no more placeholders\n",
    "    \"\"\"\n",
    "    current_text = redacted_text\n",
    "    \n",
    "    # 1) Optionally gather external context from Wikipedia\n",
    "    if wikipedia_search_terms:\n",
    "        ext_context = gather_wikipedia_context(wikipedia_search_terms, max_chars=1200)\n",
    "        # 2) Combine them\n",
    "        combined_input = f\"{current_text}\\n\\nAdditionalContext:\\n{ext_context}\"\n",
    "    else:\n",
    "        combined_input = current_text\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # 3) Summarize if needed\n",
    "        short_text = summarize_if_needed(combined_input, max_bert_length=512)\n",
    "        \n",
    "        # If there's no [REDACTED] left, break\n",
    "        if \"[REDACTED]\" not in short_text:\n",
    "            print(f\"No more [REDACTED] placeholders at epoch {epoch}. Done.\")\n",
    "            break\n",
    "        \n",
    "        # 4) Replace [REDACTED] with [MASK] (1 occurrence)\n",
    "        masked_text = short_text.replace(\"[REDACTED]\", \"[MASK]\", 1)\n",
    "        \n",
    "        # 5) BERT tokenization\n",
    "        inputs = bert_tokenizer.encode_plus(masked_text, return_tensors='pt')\n",
    "        input_ids = inputs[\"input_ids\"]\n",
    "        \n",
    "        # Identify [MASK] location\n",
    "        mask_indices = (input_ids == bert_tokenizer.mask_token_id).nonzero(as_tuple=True)[1]\n",
    "        if mask_indices.size(0) == 0:\n",
    "            print(f\"No [MASK] found at epoch {epoch}.\")\n",
    "            break\n",
    "        \n",
    "        # Forward pass\n",
    "        with torch.no_grad():\n",
    "            outputs = bert_masked_model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # 6) Get top predicted token\n",
    "        mask_logits = logits[0, mask_indices, :]\n",
    "        top_id = torch.argmax(mask_logits, dim=-1)\n",
    "        predicted_token = bert_tokenizer.decode(top_id).strip()\n",
    "        \n",
    "        # Insert predicted token\n",
    "        updated_text = masked_text.replace(\"[MASK]\", predicted_token, 1)\n",
    "        \n",
    "        combined_input = updated_text  # update our text for the next epoch\n",
    "        \n",
    "        print(f\"Epoch {epoch+1} => predicted: '{predicted_token}'\\nResult:\\n{combined_input}\\n{'-'*40}\")\n",
    "    \n",
    "    return combined_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Inference and top-K predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_top_predictions_for_masked_sentence(tokenizer, model, masked_sentence, top_k=5):\n",
    "#     model.eval()\n",
    "#     inputs = tokenizer(masked_sentence, return_tensors=\"pt\")\n",
    "#     input_ids = inputs[\"input_ids\"].to(DEVICE)\n",
    "#     attention_mask = inputs[\"attention_mask\"].to(DEVICE)\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    \n",
    "#     logits = outputs.logits  # [batch_size, seq_len, vocab_size]\n",
    "#     mask_token_index = (input_ids == tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]\n",
    "\n",
    "#     predictions = {}\n",
    "#     for idx in mask_token_index:\n",
    "#         idx = idx.item()\n",
    "#         logits_for_mask = logits[0, idx]\n",
    "#         probs = F.softmax(logits_for_mask, dim=0)\n",
    "#         top_probs, top_ids = probs.topk(top_k)\n",
    "\n",
    "#         predicted_tokens = [tokenizer.convert_ids_to_tokens(int(i)) for i in top_ids]\n",
    "#         predicted_scores = [float(tp) for tp in top_probs]\n",
    "\n",
    "#         predictions[idx] = list(zip(predicted_tokens, predicted_scores))\n",
    "\n",
    "#     return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Putting everything together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Data Example:\n",
      "{'original_text': 'The small number of [REDACTED] troops initially deployed on the [REDACTED] held long enough for the mobilised force to get into position.', 'simple_tokens': ['The', 'small', 'number', 'of', '[', 'REDACTED', ']', 'troops', 'initially', 'deployed', 'on', 'the', '[', 'REDACTED', ']', 'held', 'long', 'enough', 'for', 'the', 'mobilised', 'force', 'to', 'get', 'into', 'position', '.'], 'input_ids': tensor([[  101,  1996,  2235,  2193,  1997,  1031,  2417, 18908,  2098,  1033,\n",
      "          3629,  3322,  7333,  2006,  1996,  1031,  2417, 18908,  2098,  1033,\n",
      "          2218,  2146,  2438,  2005,  1996, 11240, 21758,  2486,  2000,  2131,\n",
      "          2046,  2597,  1012,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "[WARN] Could not retrieve Wikipedia for term 'air force' -> Page id \"air for e\" does not match any pages. Try another id!\n",
      "Epoch 1 => predicted: 'israeli'\n",
      "Result:\n",
      "The small number of israeli troops initially deployed on the [REDACTED] held long enough for the mobilised force to get into position.\n",
      "\n",
      "AdditionalContext:\n",
      "This page is subject to the extended confirmed restriction related to the Arab-Israeli conflict.\n",
      "\n",
      "The Israeli Ground Forces (Hebrew: זרוע היבשה, romanized: z'róa hibshá, lit. 'Land arm') are the ground forces of the Israel Defense Forces (IDF). The commander is the General Officer Commanding with the rank of major general, the Mazi, subordinate to the Chief of General Staff. This page is subject to the extended confirmed restriction related to the Arab-Israeli conflict.\n",
      "This page is subject to the extended confirmed restriction related to the Arab-Israeli conflict.\n",
      "\n",
      "The Golan Heights, or simply the Golan, is a basaltic plateau at the southwest corner of Syria.\n",
      "----------------------------------------\n",
      "Epoch 2 => predicted: 'ground'\n",
      "Result:\n",
      "The small number of israeli troops initially deployed on the ground held long enough for the mobilised force to get into position.\n",
      "\n",
      "AdditionalContext:\n",
      "This page is subject to the extended confirmed restriction related to the Arab-Israeli conflict.\n",
      "\n",
      "The Israeli Ground Forces (Hebrew: זרוע היבשה, romanized: z'róa hibshá, lit. 'Land arm') are the ground forces of the Israel Defense Forces (IDF). The commander is the General Officer Commanding with the rank of major general, the Mazi, subordinate to the Chief of General Staff. This page is subject to the extended confirmed restriction related to the Arab-Israeli conflict.\n",
      "This page is subject to the extended confirmed restriction related to the Arab-Israeli conflict.\n",
      "\n",
      "The Golan Heights, or simply the Golan, is a basaltic plateau at the southwest corner of Syria.\n",
      "----------------------------------------\n",
      "No more [REDACTED] placeholders at epoch 2. Done.\n",
      "\n",
      "Final Decensored Text:\n",
      " The small number of israeli troops initially deployed on the ground held long enough for the mobilised force to get into position.\n",
      "\n",
      "AdditionalContext:\n",
      "This page is subject to the extended confirmed restriction related to the Arab-Israeli conflict.\n",
      "\n",
      "The Israeli Ground Forces (Hebrew: זרוע היבשה, romanized: z'róa hibshá, lit. 'Land arm') are the ground forces of the Israel Defense Forces (IDF). The commander is the General Officer Commanding with the rank of major general, the Mazi, subordinate to the Chief of General Staff. This page is subject to the extended confirmed restriction related to the Arab-Israeli conflict.\n",
      "This page is subject to the extended confirmed restriction related to the Arab-Israeli conflict.\n",
      "\n",
      "The Golan Heights, or simply the Golan, is a basaltic plateau at the southwest corner of Syria.\n"
     ]
    }
   ],
   "source": [
    "# def main(json_file=\"./data/processed/document_1_processed.json\"):\n",
    "#     # Step A: gather external context\n",
    "#     external_context = gather_external_context_from_title(\n",
    "#         json_file,\n",
    "#         api_key=GOOGLE_API_KEY,\n",
    "#         cse_id=GOOGLE_CSE_ID\n",
    "#     )\n",
    "\n",
    "#     # Step B: load data\n",
    "#     original_sents, censored_sents = load_training_data(json_file)\n",
    "\n",
    "#     # Merge external context\n",
    "#     # We treat the external context as an additional \"sentence\" for training\n",
    "#     original_plus_context = original_sents + [external_context]\n",
    "\n",
    "#     # Step C: create masked data\n",
    "#     masked_texts, ground_truths = create_masked_texts(original_plus_context)\n",
    "\n",
    "#     # Step D: prepare & fine-tune BERT\n",
    "#     tokenizer, model = prepare_bert_model()\n",
    "#     model = fine_tune_bert_maskedLM(tokenizer, model, masked_texts, epochs=1, batch_size=2)\n",
    "\n",
    "#     # Step E: test with a random masked sentence\n",
    "#     test_masked = \"areas where the [MASK] and the [MASK] attacked.\"\n",
    "#     top_preds = get_top_predictions_for_masked_sentence(tokenizer, model, test_masked, top_k=5)\n",
    "#     print(\"\\n=== TOP PREDICTIONS FOR TEST MASK ===\")\n",
    "#     print(top_preds)\n",
    "\n",
    "#     # Also see how it tries to reconstruct a censored sentence\n",
    "#     # We do a quick hack: replace [REDACTED] with [MASK]\n",
    "#     for i, cens in enumerate(censored_sents[:3]):\n",
    "#         test_sent_masked = cens.replace(\"[REDACTED]\", tokenizer.mask_token)\n",
    "#         results = get_top_predictions_for_masked_sentence(tokenizer, model, test_sent_masked, top_k=5)\n",
    "#         print(f\"\\nCensored Sentence {i}: {cens}\")\n",
    "#         print(f\"Mask Predictions: {results}\")\n",
    "    \n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n",
    "\n",
    "###############################################\n",
    "# EXAMPLE USAGE (Just for demonstration)\n",
    "###############################################\n",
    "if __name__ == \"__main__\":\n",
    "    # Some example redacted lines\n",
    "    redacted_context_list_example = [\n",
    "        \"The small number of [REDACTED] troops initially deployed on the [REDACTED] held long enough for the mobilised force to get into position.\"\n",
    "    ]\n",
    "    \n",
    "    # Section 2: tokenization\n",
    "    tokenized_data_example = tokenize_redacted_context(redacted_context_list_example)\n",
    "    print(\"Tokenized Data Example:\")\n",
    "    for entry in tokenized_data_example:\n",
    "        print(entry)\n",
    "    \n",
    "    # Combine into single text if that's what your workflow does\n",
    "    combined_redacted_text = \" \".join(d[\"original_text\"] for d in tokenized_data_example)\n",
    "    \n",
    "    # Potential Wikipedia terms\n",
    "    wiki_terms = [\"Israeli Army\", \"air force\", \"Golan Heights\"]\n",
    "    \n",
    "    # Section 4: iterative decensoring with external context\n",
    "    final_decensored = iterative_decensoring(\n",
    "        redacted_text=combined_redacted_text,\n",
    "        wikipedia_search_terms=wiki_terms,  # pass None if you don't want external\n",
    "        epochs=3\n",
    "    )\n",
    "    print(\"\\nFinal Decensored Text:\\n\", final_decensored)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
